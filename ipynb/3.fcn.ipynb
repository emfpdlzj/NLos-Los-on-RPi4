{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uwb_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, BatchNormalization, Activation, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import raw data\n",
    "data = uwb_dataset.import_from_files()\n",
    "csv_logger = CSVLogger('result/log.csv', append=False)\n",
    "\n",
    "# Preamble(프리앰블): 통신에서 신호의 시작을 알리고, 수신 동기를 맞추기 위해 보내는 특별한 비트 패턴\n",
    "# divide CIR by RX preable count (get CIR of single preamble pulse)\n",
    "# item[2] represents number of acquired preamble symbols\n",
    "for item in data:\n",
    "    item[15:] = item[15:]/float(item[2])\n",
    "\n",
    "# test data\n",
    "train = data[:30000, :] #앞 에서부터 30000개와 모든열을 가져와 잘라냄  (2차원)\n",
    "np.random.shuffle(train) #랜덤으로 섞음.\n",
    "x_train = train[:30000, 15:] #train의 앞에서부터 30000개 row, 각 row의 15번부터 끝까지 슬라이싱. \n",
    "y_train = train[:30000, 0] #앞 에서부터 30000개와 0개의 열을 가져와 잘라냄 (1차원)\n",
    "x_test = data[30000:, 15:]\n",
    "y_test = data[30000:, 0]\n",
    "\n",
    "# feed data 검증데이터 \n",
    "x_val = x_train[25000:] #x_val은 x_train의 250000번째부터 끝까지\n",
    "y_val = y_train[25000:]\n",
    "x_train = x_train[:25000] #x_train의 앞에서 24999까지만 남김. \n",
    "y_train = y_train[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nnew=[]\n",
    "for item in x_train:\n",
    "    item = item[max([0,item.argmax()-50]) : item.argmax()+50] \n",
    "    #신호가 강한 구간 기준 앞 뒤로 50개씩 총100개 구간으로 자른다. \n",
    "    Nnew.append(item)\n",
    "x_train = np.asarray(Nnew) \n",
    "# np.assary(): 리스트를 numpy배열로 변환하는 함수\n",
    "\n",
    "Nnew=[]\n",
    "for item in x_test:\n",
    "    item = item[max([0,item.argmax()-50]) : item.argmax()+50]\n",
    "    Nnew.append(item)\n",
    "x_test = np.asarray(Nnew)\n",
    "\n",
    "Nnew=[]\n",
    "for item in x_val:\n",
    "    item = item[max([0,item.argmax()-50]) : item.argmax()+50]\n",
    "    Nnew.append(item)\n",
    "x_val = np.asarray(Nnew)\n",
    "\n",
    "# x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)).transpose(2,0,1)\n",
    "# x_val = np.reshape(x_val, (x_val.shape[0], x_val.shape[1], 1)).transpose(2,0,1)\n",
    "# x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1)).transpose(2,0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total 1016 data for CIR\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 9, strides=1, padding='valid', input_shape=(100, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(64, 7, strides=1, padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "model.add(Conv1D(128, 5, strides=1, padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "#CNN기반의 분류 모델 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_val, y_val), callbacks=[csv_logger])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "# batch_size: 정확도를 측정하고 가중치와 편향을 업데이트 하기 전에 네트워크에 공급할 훈련 데이터의 수를 지정\n",
    "# 16또는 32로 시작해서 효과적인값을 실험해보는게 좋음.\n",
    "print('## evaluation loss and metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro avg: 클래스마다 F1, Precision, Recall을 구한 뒤, 단순 평균\n",
    "# weighted avg: 각 클래스의 F1, Precision, Recall에 해당 클래스의 샘플 수로 가중치를 줘서 평균 낸 것\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 시각화 - confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1], yticklabels=[0,1])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "model.save(\"3.fcn.h5\");"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
